{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import pandas\n",
    "import subprocess\n",
    "import shlex\n",
    "\n",
    "SSH_USER = 'am72ghiassi'\n",
    "DESIGN_CSV = 'experiments.csv' # The CSV with the experiment design\n",
    "masterip = 'spark://10.132.0.4:7077'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate a random experiment ID\n",
    "ex_id = ''.join(random.choice(string.ascii_lowercase)\n",
    "                for i in range(8))\n",
    "print(f\"Experiment ID: {ex_id}\")\n",
    "\n",
    "experiments = pandas.read_csv(DESIGN_CSV)\n",
    "experiments.columns.values[0] = 'Index'\n",
    "experiments.set_index('Index')\n",
    "\n",
    "os.makedirs(f\"raw/{ex_id}\", exist_ok=True)\n",
    "shutil.copyfile(DESIGN_CSV, f\"raw/{ex_id}/design.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in experiments.iterrows():\n",
    "    print(f\"Experiment {row['Index']}\")\n",
    "    batch_size = int(row['batch_size'])\n",
    "    max_epochs = 5\n",
    "#     Create the command to start the job\n",
    "    filename = f\"{int(row['Index'])}-nodes{int(row['num_nodes'])}-batch{batch_size}-epochs{max_epochs}\"\n",
    "    command = f\"/home/{SSH_USER}/bd/spark/bin/spark-submit \"\n",
    "    command += f\"--master {masterip} --driver-cores 1 \"\n",
    "    command += f\"--driver-memory 1G --total-executor-cores {int(row['total_executor_cores'])} --executor-cores {int(row['executor_cores'])} --executor-memory {row['memory_size']} \"\n",
    "    command += f\"--py-files \\\"/home/{SSH_USER}/bd/spark/lib/bigdl-0.11.0-python-api.zip\\\" \"\n",
    "    command += f\"--properties-file \\\"/home/{SSH_USER}/bd/spark/conf/spark-bigdl.conf\\\" \"\n",
    "    command += f\"--jars \\\"/home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar\\\" \"\n",
    "    command += f\"--conf \\\"spark.driver.extraClassPath=/home/{SSH_USER}/bd/spark/lib/bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar\\\" \"\n",
    "    command += f\"--conf \\\"spark.executer.extraClassPath=bigdl-SPARK_2.3-0.11.0-jar-with-dependencies.jar,/home/{SSH_USER}/bd/codes/bi-rnn.py\\\" \"\n",
    "    command += f\"/home/{SSH_USER}/bd/codes/{row['model']}.py \"\n",
    "    command += f\"--action train --dataPath \\\"/tmp/mnist\\\" --batchSize {batch_size} --endTriggerNum {max_epochs} \"\n",
    "    command += f\"--learningRate 0.01 --learningrateDecay 0.0002\"\n",
    "#     Split the command using a shell utility for the subprocess call\n",
    "    splitted = shlex.split(command)\n",
    "#     Create output files\n",
    "    f_out = open(f\"raw/{ex_id}/{ex_id}-{filename}.out\", 'w')\n",
    "    f_err = open(f\"raw/{ex_id}/{ex_id}-{filename}.err\", 'w')\n",
    "    \n",
    "#     Run the job\n",
    "    process = subprocess.run(splitted,\n",
    "                     stdout=f_out, \n",
    "                     stderr=f_err,\n",
    "                     universal_newlines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
